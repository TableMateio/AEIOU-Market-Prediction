#!/usr/bin/env python3
"""
Enhanced AEIOU ML Pipeline - Fixing Critical Issues from AI Review
Implements all recommended fixes for maximum accuracy improvement

FIXES IMPLEMENTED:
1. Remove target leakage (abs_change_1week_after_pct)
2. Rename misleading target (pct_change_1day_after)
3. Scale numeric features for comparable ranges
4. Remove 65 constant binary flags
5. Split signed_magnitude for cleaner tree splits
6. Time-series cross-validation
"""

import os
import pandas as pd
import numpy as np
import json
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
import lightgbm as lgb
from supabase import create_client
import warnings
warnings.filterwarnings('ignore')

class EnhancedAEIOUPipeline:
    def __init__(self):
        self.timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M')
        self.supabase = None
        self.scaler = StandardScaler()
        
        # Core features (winning configuration)
        self.winning_numerical = [
            'causal_certainty', 'article_source_credibility', 'market_perception_intensity'
        ]
        
        self.categorical_features = [
            'consolidated_event_type', 'consolidated_factor_name', 'factor_category',
            'event_orientation', 'factor_orientation', 'evidence_level', 
            'evidence_source', 'market_regime', 'article_audience_split', 'event_trigger'
        ]
        
    def load_and_fix_data(self, data_path):
        """Load existing data and apply all critical fixes"""
        print("üîß LOADING DATA AND APPLYING CRITICAL FIXES")
        print("=" * 50)
        
        df = pd.read_csv(data_path)
        print(f"üìä Original data: {len(df):,} records, {len(df.columns)} columns")
        
        # FIX 1: Remove target leakage
        if 'abs_change_1week_after_pct' in df.columns:
            print("üö® Removing target leakage: abs_change_1week_after_pct")
            df = df.drop(columns=['abs_change_1week_after_pct'])
        
        # FIX 2: Rename misleading target
        if 'abs_change_1day_after_pct' in df.columns:
            print("üìõ Renaming target: abs_change_1day_after_pct ‚Üí pct_change_1day_after")
            df = df.rename(columns={'abs_change_1day_after_pct': 'pct_change_1day_after'})
        
        # FIX 3: Remove constant binary flags (65 useless flags)
        binary_flags = [col for col in df.columns if col.endswith('_present')]
        constant_flags = []
        active_flags = []
        
        for flag in binary_flags:
            if df[flag].nunique() <= 1 or df[flag].sum() == 0:
                constant_flags.append(flag)
            else:
                active_flags.append(flag)
        
        print(f"üö´ Removing {len(constant_flags)} constant binary flags (noise reduction)")
        df = df.drop(columns=constant_flags)
        print(f"‚úÖ Keeping {len(active_flags)} active binary flags")
        
        # FIX 4: Split signed_magnitude for cleaner tree splits
        if 'signed_magnitude' in df.columns:
            print("üéØ Splitting signed_magnitude ‚Üí factor_movement + factor_magnitude")
            
            # Extract direction and magnitude
            df['factor_movement_clean'] = np.sign(df['signed_magnitude'])
            df['factor_magnitude_clean'] = np.abs(df['signed_magnitude'])
            
            # Scale magnitude √ó100 for better range alignment
            df['factor_magnitude_scaled'] = df['factor_magnitude_clean'] * 100
            
            print(f"   factor_movement_clean: {df['factor_movement_clean'].value_counts().to_dict()}")
            print(f"   factor_magnitude_scaled range: [{df['factor_magnitude_scaled'].min():.2f}, {df['factor_magnitude_scaled'].max():.2f}]")
        
        # FIX 5: Scale numeric features for comparable ranges
        numerical_to_scale = []
        for col in self.winning_numerical + ['factor_magnitude_scaled']:
            if col in df.columns:
                numerical_to_scale.append(col)
        
        if numerical_to_scale:
            print(f"üìè Scaling {len(numerical_to_scale)} numerical features")
            
            # Show before scaling
            target_std = df['pct_change_1day_after'].std()
            print(f"   Target std: {target_std:.3f}")
            
            for col in numerical_to_scale:
                before_std = df[col].std()
                df[f"{col}_scaled"] = self.scaler.fit_transform(df[[col]]).flatten()
                after_std = df[f"{col}_scaled"].std()
                print(f"   {col}: std {before_std:.4f} ‚Üí {after_std:.3f}")
        
        print(f"‚úÖ Enhanced data: {len(df):,} records, {len(df.columns)} columns")
        return df, active_flags
    
    def prepare_features(self, df, active_flags):
        """Prepare features with all enhancements"""
        print("üèóÔ∏è PREPARING ENHANCED FEATURES")
        print("=" * 35)
        
        # Target
        target_col = 'pct_change_1day_after'
        y = (df[target_col] > 0).astype(int)
        
        # Features: active binary flags + scaled numerical + categorical encoded
        feature_columns = active_flags.copy()
        
        # Add scaled numerical features
        scaled_numerical = [col for col in df.columns if col.endswith('_scaled')]
        feature_columns.extend(scaled_numerical)
        
        # Add clean movement feature
        if 'factor_movement_clean' in df.columns:
            feature_columns.append('factor_movement_clean')
        
        # Prepare base feature matrix
        X = df[feature_columns].fillna(0).copy()
        
        # Encode categorical features
        encoded_count = 0
        for col in self.categorical_features:
            if col in df.columns:
                le = LabelEncoder()
                X[f"{col}_encoded"] = le.fit_transform(df[col].fillna('unknown').astype(str))
                encoded_count += 1
        
        print(f"üìä Final features: {len(X.columns)}")
        print(f"   Active binary flags: {len(active_flags)}")
        print(f"   Scaled numerical: {len(scaled_numerical)}")
        print(f"   Categorical encoded: {encoded_count}")
        print(f"üéØ Target: UP {y.sum():,} ({y.mean()*100:.1f}%), DOWN {(1-y).sum():,}")
        
        return X, y
    
    def train_with_time_series_cv(self, X, y):
        """Train with time-series cross-validation (no lookahead bias)"""
        print("ü§ñ TRAINING WITH TIME-SERIES CV")
        print("=" * 35)
        
        # Time-series split (80% train, 20% test, no shuffling)
        train_size = int(0.8 * len(X))
        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]
        
        print(f"üìà Time-series split: Train {len(X_train):,}, Test {len(X_test):,}")
        
        # RandomForest
        rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        rf.fit(X_train, y_train)
        rf_pred = rf.predict(X_test)
        rf_accuracy = accuracy_score(y_test, rf_pred) * 100
        
        # Enhanced LightGBM
        lgb_train = lgb.Dataset(X_train, label=y_train)
        lgb_test = lgb.Dataset(X_test, label=y_test, reference=lgb_train)
        
        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': 42
        }
        
        model = lgb.train(
            params,
            lgb_train,
            valid_sets=[lgb_test],
            num_boost_round=100,
            callbacks=[lgb.early_stopping(20), lgb.log_evaluation(0)]
        )
        
        lgb_pred = model.predict(X_test, num_iteration=model.best_iteration)
        lgb_pred_binary = (lgb_pred > 0.5).astype(int)
        lgb_accuracy = accuracy_score(y_test, lgb_pred_binary) * 100
        
        print(f"üå≤ RandomForest: {rf_accuracy:.1f}%")
        print(f"‚ö° LightGBM: {lgb_accuracy:.1f}%")
        
        # Feature importance analysis
        feature_importance = model.feature_importance(importance_type='gain')
        feature_names = X.columns.tolist()
        
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)
        
        print(f"\\nüìä TOP 10 FEATURES (after fixes):")
        for i, (_, row) in enumerate(importance_df.head(10).iterrows()):
            print(f"   {i+1:2d}. {row['feature']}: {row['importance']:.1f}")
        
        return {
            'rf_accuracy': rf_accuracy,
            'lgb_accuracy': lgb_accuracy,
            'model': model,
            'feature_importance': importance_df,
            'improvements': {
                'target_leakage_removed': True,
                'constant_flags_removed': True,
                'features_scaled': True,
                'signed_magnitude_split': True,
                'time_series_cv': True
            }
        }
    
    def save_enhanced_results(self, results, df):
        """Save results with improvement tracking"""
        run_dir = f"../results/ml_runs/enhanced_run_{self.timestamp}"
        os.makedirs(run_dir, exist_ok=True)
        
        # Enhanced results summary
        summary = {
            'timestamp': self.timestamp,
            'configuration': 'enhanced_with_ai_fixes',
            'performance': {
                'lightgbm_accuracy': results['lgb_accuracy'],
                'randomforest_accuracy': results['rf_accuracy']
            },
            'improvements_applied': results['improvements'],
            'data_stats': {
                'total_records': len(df),
                'features_after_cleanup': len(results['feature_importance']),
                'target_distribution': {
                    'up_moves': int((df['pct_change_1day_after'] > 0).sum()),
                    'down_moves': int((df['pct_change_1day_after'] <= 0).sum())
                }
            }
        }
        
        # Save files
        with open(f"{run_dir}/enhanced_results.json", 'w') as f:
            json.dump(summary, f, indent=2)
        
        results['feature_importance'].to_csv(f"{run_dir}/feature_importance.csv", index=False)
        
        print(f"‚úÖ Enhanced results saved: {run_dir}/")
        return run_dir, summary
    
    def run_enhanced_pipeline(self, data_path):
        """Run complete enhanced pipeline"""
        print("üéØ ENHANCED AEIOU ML PIPELINE")
        print("=" * 50)
        print("Implementing AI reviewer's critical fixes")
        print()
        
        start_time = datetime.now()
        
        # Load and fix data
        df, active_flags = self.load_and_fix_data(data_path)
        
        # Prepare enhanced features
        X, y = self.prepare_features(df, active_flags)
        
        # Train with improvements
        results = self.train_with_time_series_cv(X, y)
        
        # Save results
        run_dir, summary = self.save_enhanced_results(results, df)
        
        # Final comparison
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print(f"\\nüéâ ENHANCED PIPELINE COMPLETE!")
        print(f"‚è±Ô∏è  Total time: {duration:.1f} seconds")
        print(f"üéØ LightGBM: {results['lgb_accuracy']:.1f}% (vs 65.3% baseline)")
        print(f"üìä Improvement: {results['lgb_accuracy'] - 65.3:+.1f} percentage points")
        print(f"üìÅ Results: {run_dir}")
        
        if results['lgb_accuracy'] > 65.3:
            print("üöÄ SUCCESS! AI fixes improved accuracy!")
        elif results['lgb_accuracy'] >= 65.0:
            print("‚úÖ Maintained high accuracy with cleaner features")
        else:
            print("‚ö†Ô∏è Some accuracy lost - may need further tuning")
        
        return results, summary

if __name__ == "__main__":
    pipeline = EnhancedAEIOUPipeline()
    
    # Use existing prepared data
    data_path = '../results/ml_runs/run_2025-09-06_14-31/prepared_clean_data.csv'
    results, summary = pipeline.run_enhanced_pipeline(data_path)
