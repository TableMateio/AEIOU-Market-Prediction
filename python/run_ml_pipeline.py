#!/usr/bin/env python3
"""
AEIOU ML Pipeline Runner
Single script to export data and run ML pipeline with proper organization
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from feature_config import FEATURE_CONFIG
import subprocess
import json

class AEIOUPipelineRunner:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.data_dir = self.project_root / "data" / "exports"
        self.results_dir = self.project_root / "results" / "ml_runs"
        
        # Create directories if they don't exist
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def export_training_data(self) -> str:
        """Export training data from Supabase with proper naming"""
        
        print("ğŸš€ EXPORTING TRAINING DATA FROM SUPABASE")
        print("=" * 50)
        
        # Create timestamped filename
        export_filename = f"apple_training_data_{self.timestamp}.csv"
        export_path = self.data_dir / export_filename
        
        print(f"ğŸ“Š Target: {FEATURE_CONFIG.primary_target}")
        print(f"ğŸ“ Export path: {export_path}")
        
        # SQL query to get all the data we need
        sql_query = f"""
        SELECT 
          article_id,
          article_published_at,
          
          -- Consolidated features
          consolidated_event_tags,
          consolidated_event_type,
          consolidated_factor_name,
          event_tag_category,
          factor_category,
          
          -- Core numerical features
          factor_magnitude,
          factor_movement,
          causal_certainty,
          article_source_credibility,
          market_perception_intensity,
          
          -- Extended numerical features (check availability)
          market_perception_hope_vs_fear,
          market_perception_surprise_vs_anticipated,
          market_perception_consensus_vs_division,
          market_perception_narrative_strength,
          ai_assessment_execution_risk,
          ai_assessment_competitive_risk,
          ai_assessment_business_impact_likelihood,
          ai_assessment_timeline_realism,
          ai_assessment_fundamental_strength,
          perception_gap_optimism_bias,
          perception_gap_risk_awareness,
          perception_gap_correction_potential,
          regime_alignment,
          reframing_potential,
          narrative_disruption,
          logical_directness,
          market_consensus_on_causality,
          article_author_credibility,
          article_publisher_credibility,
          article_time_lag_days,
          
          -- Price data for target calculation
          price_at_event,
          price_1day_after,
          price_1week_after
          
        FROM ml_training_data 
        WHERE consolidated_factor_name IS NOT NULL
        AND price_at_event IS NOT NULL
        AND price_1day_after IS NOT NULL
        ORDER BY article_published_at ASC;
        """
        
        print("ğŸ”„ Running MCP query...")
        print(f"ğŸ“ˆ Expected: ~9,817 records with valid targets")
        
        # Actually export the data using MCP
        try:
            print("ğŸ”„ Running MCP query to get REAL data with correct targets...")
            print(f"ğŸ¯ Looking for: {FEATURE_CONFIG.primary_target} and {FEATURE_CONFIG.secondary_target}")
            
            # TODO: Replace this with actual MCP call using the SQL from export_real_apple_data.py
            print("âš ï¸  MCP integration needed - using existing CSV for now")
            
            # Use sample data with correct target structure
            import shutil
            sample_csv = "/Users/scottbergman/Dropbox/Projects/AEIOU/data/exports/apple_real_data_sample_20250906.csv"
            if os.path.exists(sample_csv):
                shutil.copy(sample_csv, export_path)
                print(f"âœ… Sample data with CORRECT TARGETS copied to: {export_path}")
                print(f"ğŸ¯ Contains: {FEATURE_CONFIG.primary_target} and {FEATURE_CONFIG.secondary_target}")
            else:
                print(f"âŒ Could not find sample data at: {sample_csv}")
                return None
                
        except Exception as e:
            print(f"âŒ Export failed: {e}")
            return None
        
        return str(export_path)
    
    def calculate_target(self, df: pd.DataFrame) -> pd.DataFrame:
        """Set up target variables - prefer abs_change columns from database"""
        
        print("ğŸ¯ SETTING UP TARGET VARIABLE")
        
        # Check what targets are available
        available_targets = [col for col in df.columns if 'alpha' in col or 'change' in col]
        print(f"Available targets: {available_targets}")
        
        # Prefer our configured targets (abs_change_1day_after_pct, abs_change_1week_after_pct)
        if FEATURE_CONFIG.primary_target in df.columns:
            target_col = FEATURE_CONFIG.primary_target
            print(f"âœ… Using {target_col} as target (Apple's % change vs itself)")
        elif FEATURE_CONFIG.secondary_target in df.columns:
            target_col = FEATURE_CONFIG.secondary_target
            print(f"âœ… Using {target_col} as target (Apple's 1-week % change)")
        elif 'alpha_vs_spy_1day_after' in df.columns:
            target_col = 'alpha_vs_spy_1day_after'
            print(f"âš ï¸  Primary targets not found. Using {target_col} as fallback (Apple's alpha vs SPY)")
        else:
            print("âŒ No suitable target found!")
            return df
        
        # Set the target (copy to primary_target name for consistency)
        if target_col != FEATURE_CONFIG.primary_target:
            df[FEATURE_CONFIG.primary_target] = df[target_col]
        
        # Print target distribution
        target_data = df[FEATURE_CONFIG.primary_target].dropna()
        print(f"ğŸ“Š TARGET DISTRIBUTION:")
        print(f"  Count: {len(target_data)}")
        print(f"  Mean:  {target_data.mean():.3f}%")
        print(f"  Std:   {target_data.std():.3f}%")
        print(f"  Range: [{target_data.min():.3f}, {target_data.max():.3f}]")
        
        # Binary classification stats
        positive_moves = (target_data > 0).sum()
        negative_moves = (target_data < 0).sum()
        print(f"  Positive moves: {positive_moves} ({positive_moves/len(target_data)*100:.1f}%)")
        print(f"  Negative moves: {negative_moves} ({negative_moves/len(target_data)*100:.1f}%)")
        
        return df
    
    def create_binary_flags(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create binary flags from array-type fields"""
        
        print("ğŸ—ï¸  CREATING BINARY FLAGS")
        
        # 1. Event Tags
        if 'consolidated_event_tags' in df.columns:
            for tag in FEATURE_CONFIG.consolidated_event_tags:
                flag_name = f"{tag}_tag_present"
                df[flag_name] = df['consolidated_event_tags'].apply(
                    lambda x: 1 if x and tag in str(x) else 0
                )
            print(f"âœ… Created {len(FEATURE_CONFIG.consolidated_event_tags)} event tag flags")
        else:
            print("âš ï¸  'consolidated_event_tags' column not found")
        
        # 2. Emotional Profile Flags
        if 'market_perception_emotional_profile' in df.columns:
            for emotion in FEATURE_CONFIG.emotional_profile_values:
                flag_name = f"emotion_{emotion}_present"
                df[flag_name] = df['market_perception_emotional_profile'].apply(
                    lambda x: 1 if x and emotion in str(x) else 0
                )
            print(f"âœ… Created {len(FEATURE_CONFIG.emotional_profile_values)} emotional profile flags")
        else:
            print("âš ï¸  'market_perception_emotional_profile' column not found")
            # Create empty flags for missing data
            for emotion in FEATURE_CONFIG.emotional_profile_values:
                df[f"emotion_{emotion}_present"] = 0
        
        # 3. Cognitive Bias Flags
        if 'market_perception_cognitive_biases' in df.columns:
            for bias in FEATURE_CONFIG.cognitive_biases_values:
                flag_name = f"bias_{bias}_present"
                df[flag_name] = df['market_perception_cognitive_biases'].apply(
                    lambda x: 1 if x and bias in str(x) else 0
                )
            print(f"âœ… Created {len(FEATURE_CONFIG.cognitive_biases_values)} cognitive bias flags")
        else:
            print("âš ï¸  'market_perception_cognitive_biases' column not found")
            # Create empty flags for missing data
            for bias in FEATURE_CONFIG.cognitive_biases_values:
                df[f"bias_{bias}_present"] = 0
        
        total_flags = len(FEATURE_CONFIG.get_all_binary_flags())
        print(f"ğŸ¯ TOTAL BINARY FLAGS CREATED: {total_flags}")
        return df
    
    def run_ml_models(self, data_path: str) -> str:
        """Run ML models on the prepared data with full analysis"""
        
        print("ğŸ¤– RUNNING ML MODELS")
        print("=" * 50)
        
        # Create results directory for this run
        run_results_dir = self.results_dir / f"run_{self.timestamp}"
        run_results_dir.mkdir(exist_ok=True)
        
        # Load and prepare data
        df = pd.read_csv(data_path)
        print(f"ğŸ“ˆ Loaded {len(df)} training examples")
        
        # Calculate target
        df = self.calculate_target(df)
        
        # Create binary flags
        df = self.create_binary_flags(df)
        
        # Save prepared data
        prepared_data_path = run_results_dir / "prepared_data.csv"
        df.to_csv(prepared_data_path, index=False)
        print(f"ğŸ’¾ Prepared data saved: {prepared_data_path}")
        
        # Import and run the actual ML pipeline
        print("ğŸ”„ Running ML models...")
        
        from sklearn.model_selection import train_test_split
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        import lightgbm as lgb
        
        # Prepare features - use ALL features including new ones
        feature_columns = (
            FEATURE_CONFIG.categorical_features +
            FEATURE_CONFIG.get_all_numerical_features() +
            FEATURE_CONFIG.get_all_binary_flags()  # This includes event tags + emotions + biases
        )
        
        # Filter to available columns
        available_features = [col for col in feature_columns if col in df.columns]
        missing_features = [col for col in feature_columns if col not in df.columns]
        
        print(f"âœ… Available features: {len(available_features)}")
        print(f"âš ï¸  Missing features: {len(missing_features)}")
        
        # Prepare data for ML
        X = df[available_features].fillna(0)  # Simple fillna for now
        y = df[FEATURE_CONFIG.primary_target].fillna(0)
        
        # Encode categorical features
        from sklearn.preprocessing import LabelEncoder
        label_encoders = {}
        for col in FEATURE_CONFIG.categorical_features:
            if col in X.columns:
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))
                label_encoders[col] = le
        
        # Time-based split if we have timestamps
        if 'article_published_at' in df.columns:
            df_sorted = df.sort_values('article_published_at')
            split_idx = int(len(df_sorted) * 0.8)
            train_idx = df_sorted.index[:split_idx]
            test_idx = df_sorted.index[split_idx:]
            
            X_train, X_test = X.loc[train_idx], X.loc[test_idx]
            y_train, y_test = y.loc[train_idx], y.loc[test_idx]
            print("ğŸ“… Using time-based split (80% train, 20% test)")
        else:
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            print("ğŸ² Using random split (80% train, 20% test)")
        
        print(f"ğŸ“Š Train: {len(X_train)}, Test: {len(X_test)}")
        
        # Train models
        models = {}
        results = {}
        
        # 1. Random Forest
        print("ğŸŒ² Training Random Forest...")
        rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
        rf.fit(X_train, y_train)
        rf_pred = rf.predict(X_test)
        
        models['random_forest'] = rf
        results['random_forest'] = {
            'rmse': np.sqrt(mean_squared_error(y_test, rf_pred)),
            'mae': mean_absolute_error(y_test, rf_pred),
            'r2': r2_score(y_test, rf_pred),
            'directional_accuracy': ((rf_pred > 0) == (y_test > 0)).mean() * 100
        }
        
        # 2. LightGBM
        print("âš¡ Training LightGBM...")
        lgb_train = lgb.Dataset(X_train, y_train)
        lgb_valid = lgb.Dataset(X_test, y_test, reference=lgb_train)
        
        lgb_params = {
            'objective': 'regression',
            'metric': 'rmse',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.1,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1
        }
        
        lgb_model = lgb.train(
            lgb_params,
            lgb_train,
            valid_sets=[lgb_valid],
            num_boost_round=100,
            callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
        )
        
        lgb_pred = lgb_model.predict(X_test)
        
        models['lightgbm'] = lgb_model
        results['lightgbm'] = {
            'rmse': np.sqrt(mean_squared_error(y_test, lgb_pred)),
            'mae': mean_absolute_error(y_test, lgb_pred),
            'r2': r2_score(y_test, lgb_pred),
            'directional_accuracy': ((lgb_pred > 0) == (y_test > 0)).mean() * 100
        }
        
        # Feature importance analysis
        print("ğŸ“Š Analyzing feature importance...")
        
        # Random Forest feature importance
        rf_importance = pd.DataFrame({
            'feature': available_features,
            'importance': rf.feature_importances_
        }).sort_values('importance', ascending=False)
        
        # LightGBM feature importance
        lgb_importance = pd.DataFrame({
            'feature': available_features,
            'importance': lgb_model.feature_importance()
        }).sort_values('importance', ascending=False)
        
        # Categorize features for analysis
        def categorize_feature(feature_name):
            if feature_name in FEATURE_CONFIG.categorical_features:
                return 'categorical'
            elif feature_name in FEATURE_CONFIG.get_all_numerical_features():
                return 'numerical'
            elif feature_name.endswith('_tag_present'):
                return 'event_tag_flag'
            elif feature_name.startswith('emotion_') and feature_name.endswith('_present'):
                return 'emotion_flag'
            elif feature_name.startswith('bias_') and feature_name.endswith('_present'):
                return 'bias_flag'
            else:
                return 'other'
        
        # Add categories to importance dataframes
        rf_importance['category'] = rf_importance['feature'].apply(categorize_feature)
        lgb_importance['category'] = lgb_importance['feature'].apply(categorize_feature)
        
        # Create category-specific rankings (keep in memory for analysis)
        rf_by_category = {}
        lgb_by_category = {}
        
        categories = ['categorical', 'numerical', 'event_tag_flag', 'emotion_flag', 'bias_flag']
        for category in categories:
            rf_cat = rf_importance[rf_importance['category'] == category].head(10)
            lgb_cat = lgb_importance[lgb_importance['category'] == category].head(10)
            
            rf_by_category[category] = rf_cat
            lgb_by_category[category] = lgb_cat
        
        # Save consolidated feature importance (one file with all info)
        with pd.ExcelWriter(run_results_dir / f"feature_importance_analysis.xlsx") as writer:
            rf_importance.to_excel(writer, sheet_name='RandomForest_All', index=False)
            lgb_importance.to_excel(writer, sheet_name='LightGBM_All', index=False)
            
            # Add category breakdowns as separate sheets
            for category in categories:
                if len(rf_by_category[category]) > 0:
                    rf_by_category[category].to_excel(writer, sheet_name=f'RF_{category}', index=False)
                if len(lgb_by_category[category]) > 0:
                    lgb_by_category[category].to_excel(writer, sheet_name=f'LGB_{category}', index=False)
        
        # Feature correlation analysis
        print("ğŸ”— Analyzing feature correlations...")
        feature_corr = X[available_features].corr()
        target_corr = X[available_features].corrwith(y).sort_values(ascending=False)
        
        # Save correlations in one Excel file
        with pd.ExcelWriter(run_results_dir / f"correlation_analysis.xlsx") as writer:
            feature_corr.to_excel(writer, sheet_name='Feature_Correlations')
            target_corr.to_frame('correlation').to_excel(writer, sheet_name='Target_Correlations')
        
        # Create comprehensive summary
        summary = {
            "run_info": {
                "timestamp": self.timestamp,
                "data_path": str(data_path),
                "prepared_data_path": str(prepared_data_path),
                "results_dir": str(run_results_dir)
            },
            "data_stats": {
                "total_records": len(df),
                "train_records": len(X_train),
                "test_records": len(X_test),
                "target_variable": FEATURE_CONFIG.primary_target,
                "available_features": len(available_features),
                "missing_features": len(missing_features)
            },
            "target_distribution": {
                "mean": float(y.mean()),
                "std": float(y.std()),
                "min": float(y.min()),
                "max": float(y.max()),
                "positive_moves": int((y > 0).sum()),
                "negative_moves": int((y < 0).sum()),
                "positive_pct": float((y > 0).mean() * 100)
            },
            "model_performance": results,
            "top_features": {
                "random_forest": rf_importance.head(10).to_dict('records'),
                "lightgbm": lgb_importance.head(10).to_dict('records')
            },
            "top_features_by_category": {
                "random_forest": {cat: df.to_dict('records') for cat, df in rf_by_category.items()},
                "lightgbm": {cat: df.to_dict('records') for cat, df in lgb_by_category.items()}
            },
                "feature_categories": {
                    "categorical": len([f for f in FEATURE_CONFIG.categorical_features if f in available_features]),
                    "numerical": len([f for f in FEATURE_CONFIG.get_all_numerical_features() if f in available_features]),
                    "event_tag_flags": len([f for f in FEATURE_CONFIG.get_binary_flag_features() if f in available_features]),
                    "emotion_flags": len([f for f in FEATURE_CONFIG.get_emotional_profile_flags() if f in available_features]),
                    "bias_flags": len([f for f in FEATURE_CONFIG.get_cognitive_bias_flags() if f in available_features]),
                    "total_binary_flags": len([f for f in FEATURE_CONFIG.get_all_binary_flags() if f in available_features])
                }
        }
        
        # Save comprehensive summary
        summary_path = run_results_dir / "COMPLETE_RESULTS.json"
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        # Create markdown summary for easy reading
        self._create_markdown_summary(run_results_dir, summary, rf_importance, lgb_importance, rf_by_category)
        
        print(f"âœ… Complete results saved to: {run_results_dir}")
        print(f"ğŸ“Š Key files:")
        print(f"  â€¢ COMPLETE_RESULTS.json - Full results data")
        print(f"  â€¢ SUMMARY.md - Easy-to-read summary")
        print(f"  â€¢ feature_importance_analysis.xlsx - All feature rankings")
        print(f"  â€¢ correlation_analysis.xlsx - All correlations")
        print(f"  â€¢ prepared_data.csv - Your processed dataset")
        
        return str(run_results_dir)
    
    def _create_markdown_summary(self, results_dir, summary, rf_importance, lgb_importance, rf_by_category):
        """Create a readable markdown summary"""
        
        md_content = f"""# AEIOU ML Results - {self.timestamp}

## ğŸ¯ Model Performance

### Random Forest
- **RMSE**: {summary['model_performance']['random_forest']['rmse']:.4f}
- **MAE**: {summary['model_performance']['random_forest']['mae']:.4f}
- **RÂ²**: {summary['model_performance']['random_forest']['r2']:.4f}
- **Directional Accuracy**: {summary['model_performance']['random_forest']['directional_accuracy']:.1f}%

### LightGBM
- **RMSE**: {summary['model_performance']['lightgbm']['rmse']:.4f}
- **MAE**: {summary['model_performance']['lightgbm']['mae']:.4f}
- **RÂ²**: {summary['model_performance']['lightgbm']['r2']:.4f}
- **Directional Accuracy**: {summary['model_performance']['lightgbm']['directional_accuracy']:.1f}%

## ğŸ“Š Data Summary
- **Total Records**: {summary['data_stats']['total_records']:,}
- **Train/Test Split**: {summary['data_stats']['train_records']:,} / {summary['data_stats']['test_records']:,}
- **Available Features**: {summary['data_stats']['available_features']}
- **Target**: {summary['data_stats']['target_variable']}

## ğŸ¯ Target Distribution
- **Mean**: {summary['target_distribution']['mean']:.3f}%
- **Std**: {summary['target_distribution']['std']:.3f}%
- **Range**: [{summary['target_distribution']['min']:.3f}, {summary['target_distribution']['max']:.3f}]
- **Positive Moves**: {summary['target_distribution']['positive_moves']} ({summary['target_distribution']['positive_pct']:.1f}%)
- **Negative Moves**: {summary['target_distribution']['negative_moves']} ({100-summary['target_distribution']['positive_pct']:.1f}%)

## ğŸ† Top 10 Features (Random Forest)
"""
        
        for i, row in rf_importance.head(10).iterrows():
            md_content += f"{i+1}. **{row['feature']}**: {row['importance']:.4f}\n"
        
        md_content += "\n## âš¡ Top 10 Features (LightGBM)\n"
        
        for i, row in lgb_importance.head(10).iterrows():
            md_content += f"{i+1}. **{row['feature']}**: {row['importance']:.0f}\n"
        
        # Add category breakdowns
        md_content += "\n## ğŸ·ï¸ Top Features by Category\n"
        
        category_display_names = {
            'event_tag_flag': 'Event Tag Flags',
            'emotion_flag': 'Emotional Profile Flags', 
            'bias_flag': 'Cognitive Bias Flags',
            'numerical': 'Numerical Features',
            'categorical': 'Categorical Features'
        }
        
        for category in ['event_tag_flag', 'emotion_flag', 'bias_flag', 'numerical', 'categorical']:
            if category in rf_by_category and len(rf_by_category[category]) > 0:
                category_name = category_display_names.get(category, category.replace('_', ' ').title())
                md_content += f"\n### {category_name} (Random Forest)\n"
                for i, (_, row) in enumerate(rf_by_category[category].head(5).iterrows(), 1):
                    md_content += f"{i}. **{row['feature']}**: {row['importance']:.4f}\n"
        
        md_content += f"""
## ğŸ“ Files Generated
- `COMPLETE_RESULTS_{self.timestamp}.json` - Full results data
- `prepared_data_{self.timestamp}.csv` - Processed training data
- `random_forest_feature_importance_{self.timestamp}.csv` - RF feature rankings
- `lightgbm_feature_importance_{self.timestamp}.csv` - LGB feature rankings
- `feature_correlations_{self.timestamp}.csv` - Feature correlation matrix
- `target_correlations_{self.timestamp}.csv` - Feature-target correlations

## ğŸ”§ Feature Breakdown
- **Categorical Features**: {summary['feature_categories']['categorical']}
- **Numerical Features**: {summary['feature_categories']['numerical']}
- **Event Tag Flags**: {summary['feature_categories']['event_tag_flags']}
- **Emotion Flags**: {summary['feature_categories']['emotion_flags']}
- **Bias Flags**: {summary['feature_categories']['bias_flags']}
- **Total Binary Flags**: {summary['feature_categories']['total_binary_flags']}
"""
        
        # Save markdown summary
        md_path = results_dir / "SUMMARY.md"
        with open(md_path, 'w') as f:
            f.write(md_content)
        
        print(f"ğŸ“ Markdown summary: SUMMARY.md")
    
    def run_complete_pipeline(self):
        """Run the complete pipeline: export -> prepare -> train -> evaluate"""
        
        print("ğŸ‰ STARTING COMPLETE AEIOU ML PIPELINE")
        print("=" * 60)
        
        # Print feature configuration
        FEATURE_CONFIG.print_feature_summary()
        print()
        
        # Step 1: Export data
        data_path = self.export_training_data()
        print()
        
        # Step 2: Run ML pipeline
        results_dir = self.run_ml_models(data_path)
        print()
        
        print("ğŸ‰ PIPELINE COMPLETE!")
        print(f"ğŸ“ Results saved in: {results_dir}")
        print(f"ğŸ“Š Data exported to: {data_path}")
        
        return {
            "data_path": data_path,
            "results_dir": results_dir,
            "timestamp": self.timestamp
        }

if __name__ == "__main__":
    runner = AEIOUPipelineRunner()
    results = runner.run_complete_pipeline()
