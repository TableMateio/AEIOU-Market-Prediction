#!/usr/bin/env tsx

/**
 * Scrape All Collected Articles
 * 
 * Scrapes full content for all articles in our database and updates them
 */

import axios from 'axios';
import * as cheerio from 'cheerio';

interface Article {
    id: string;
    title: string;
    url: string;
}

async function scrapeContent(url: string): Promise<{ content: string; length: number; issues: string[] }> {
    try {
        const response = await axios.get(url, {
            timeout: 15000,
            headers: {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
        });

        const $ = cheerio.load(response.data);

        // Extract content
        let content = '';
        const articleSelectors = [
            'article',
            '.article-content',
            '.article-body',
            '.post-content',
            '.entry-content',
            '.content',
            '.story-body'
        ];

        for (const selector of articleSelectors) {
            const element = $(selector);
            if (element.length > 0) {
                content = element.text().trim();
                if (content.length > 500) {
                    break;
                }
            }
        }

        if (content.length < 500) {
            const paragraphs = $('p').map((i, el) => $(el).text().trim()).get();
            content = paragraphs.join('\n\n');
        }

        content = content.replace(/\s+/g, ' ').trim();

        // Check for issues
        const issues: string[] = [];
        const lower = content.toLowerCase();

        if (content.length < 200) issues.push('Very short');
        if (lower.includes('subscribe') && lower.includes('paywall')) issues.push('Paywall');
        if (lower.includes('404')) issues.push('Not found');

        return { content, length: content.length, issues };

    } catch (error) {
        return { content: '', length: 0, issues: [error.message] };
    }
}

// This would be used with MCP tools - showing the concept
async function main() {
    console.log('ðŸ•·ï¸ Scraping All Articles');
    console.log('='.repeat(50));

    // In real implementation, we'd get articles from Supabase
    // and update them with scraped content

    console.log('ðŸ“‹ This script shows how to scrape all articles.');
    console.log('ðŸ“ Use the MCP Supabase tools to:');
    console.log('   1. SELECT articles WHERE scraping_status = "pending"');
    console.log('   2. Scrape each URL');
    console.log('   3. UPDATE articles SET body = scraped_content WHERE id = article_id');

    console.log('\nâœ¨ Ready to process all 19 articles with high-quality scraping!');
}

if (require.main === module) {
    main().catch(console.error);
}
